#coding=utf-8
'''
@author: zhangy
'''
import requests
from web.constant.dscrapy_constant import DescrapyConstant
class DscrapyCommand():
    
    @classmethod
    def list_projects_cmd(self, project, spider, host = '127.0.0.1', port = DescrapyConstant.DEFAULT_DSCRAPY_SERVER_PORT):
        pass
    
    @classmethod
    def nodes_status_cmd(self, project, spider, host = '127.0.0.1', port = DescrapyConstant.DEFAULT_DSCRAPY_SERVER_PORT):
        pass
    
    @classmethod
    def list_spiders_cmd(self, project, servers = ['127.0.0.1:DescrapyConstant.DEFAULT_DSCRAPY_SERVER_PORT']):
        spiders_list = []
        params = {'project':project}
        for server in servers:
            result_json = requests.post("http://" + server + '/listspiders.json', data = params)
            result_json['ip'] = server.split(":")[0]
            spiders_list.append(result_json)
        return spiders_list
        
    @classmethod
    def spiders_versions_cmd(self, project, spider, host = '127.0.0.1', port = DescrapyConstant.DEFAULT_DSCRAPY_SERVER_PORT):
        pass
    
    @classmethod
    def add_version_cmd(self, server = '127.0.0.1:DescrapyConstant.DEFAULT_DSCRAPY_SERVER_PORT'):
        params = {'project': 'dscrapy', 'version':'12333333', 'egg':'C:\Users\zhangy\eggs\dscrapy\1501595329.egg'}
        result = requests.post("http://" + server + '/addversion.json', data = params)
        result_str = result.content
        print result_str
    
    
    @classmethod
    def list_jobs_cmd(self, project, host = '127.0.0.1', port = DescrapyConstant.DEFAULT_DSCRAPY_SERVER_PORT):
        params = {'project':project}
        result_str = "{}"
        try:
            result = requests.get("http://" + host + ":" + str(port) + '/listjobs.json', params)
            result_str = result.content
        except Exception as e:
            print e
        return result_str
    
    @classmethod
    def start_job_cmd(self, project, spider, host = '127.0.0.1', port = DescrapyConstant.DEFAULT_DSCRAPY_SERVER_PORT):
        params = {'project':project, 'spider':spider}
        result_json = requests.post("http://" + host + ":" + str(port) + '/schedule.json', data = params)
        return result_json.content
        
    @classmethod
    def cancel_job_cmd(self, project, job_id, host = '127.0.0.1', port = DescrapyConstant.DEFAULT_DSCRAPY_SERVER_PORT):
        params = {'project':project, 'job':job_id}
        result_json = requests.post("http://" + host + ":" + str(port) + '/cancel.json', data = params)
        return result_json.content
    
    @classmethod
    def del_spider_cmd(self, project, spider, host = '127.0.0.1', port = DescrapyConstant.DEFAULT_DSCRAPY_SERVER_PORT):
        pass
    
    @classmethod
    def del_project_cmd(self, project, spider, host = '127.0.0.1', port = DescrapyConstant.DEFAULT_DSCRAPY_SERVER_PORT):
        pass
    
    @classmethod
    def find_log_cmd(self, project, spider_name, job_id, host = '127.0.0.1', port = DescrapyConstant.DEFAULT_DSCRAPY_SERVER_PORT):
        url = "http://" + host + ":" + str(port) + "/logs/" + project + "/" + spider_name + "/" + job_id + ".log"
        print 'url: ', url
        return requests.get(url).content
    
 
    
print DscrapyCommand.start_job_cmd('dscrapy', 'jd_detail', '127.0.0.1', 6800) 
    
    
    
    
    
    
    
    
    
    
        
        